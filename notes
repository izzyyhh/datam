VO

social media analysis
------
text analysis through posts, articles,..
preprocess text and analyse on topics, sentiments, keywords

semantic analysis
meaning from text
discover thematic structure from corpus
and annotate what it is about

topic modeling
discover topics
model like LDA latent dirichlet allocation
analyses docs to discover topics

LDA
unsupervised
man gibt nur anzahl der themen die man haben möchte an

k = num topics i want to have
beta = wv themen ein doc zugewiesen werden kann
...

sentiment analysis
irony, love like hate,... ambiguity
how do machines understand this kind of content


LSTM vs Transformer
transformer current state of the art

BERT with encoder decoder

geospatial analysis
kernel density, where are more tweets as an example
flüchtlingsströme lokalisieren zum beispiel

hotspot = werte drumherum haben auch hohe werte, gewichtung erfolgt dadurch
paper: "Spatio-temporal machine learning analysis of social media data and refugee movement statistics"




titanic
decision tree max depth
gini index to indetify how pure the branch, je kleiner desto klarer was das ergebnis ist in diesem branch
hotspot = werte drumherum haben auch hohe werte, gewichtung erfolgt dadurch



VO

neuronale netzwerke gehirn als vorbild
universell einsetzbar (supervised, unsupervised,...)

Deep Learning vs neuronale netzwerke
    neuronale netzwerke mit mehr als 2 layer ist deep learning

Deep learning unterschied zb zur linearen regression, lr input output
deep learning input output von layer zu layer wird übeergeben an anderen zum weiterverarbeiten dann output


neuronale netzwerke brauchen viel daten
neuronale netzwerke adaptieren sich gut

xg boost is competitor zu nn (auch faktorisierungsmaschine?)

features von neural networks schwer nachzuvollziehen
explainability issues

nn can be used for all multimedia data and numeric data
for
    image recognition
    speech processing
    natural language processing
    ....

for nn you have not to preprocess, but leads to better results
nn versteht selber welche features er erstellt und hernimmt

paper: random seed optimization with nn

neural netzwerk als basis lineare regression

fnn(x) = f3(f2(f1(x))) kette



aufbau
    Input layer | Hidden Layer (Neuronen) | Output Layer


activation function
    chaining linear regressions is not sufficient
    sigmoid, tanh, ReLu (eig nicht ableitbar aber geht)
    man kann selbst eine definieren zum aktivieren von Neuronen
    gradient
    in den hidden layers

brauchen ableitbare funktionen, weil wir stetig ableiten

wie funktioniert das optimieren in nn?
mit activation function und gradient

die anpassung der weights passiert erst am ende
man lässt durchlaufen, der fehler kann sich durchziehen

nicht linear machen mit der aktivierungsfunktion

im Output layer:
for classification Softmax wertebereich zw 0,1, prozente kommen raus
for regression W * x + b


loss function
for classification: cross entropy
for regression: MSE
fehler wird dem anderen layer übergeben,       zum gradient

convolotuional für image und text, cross correlation
lstms für sequenzen, zeit zb

Rekonstruktionsfehler bei Autoencoders
input und output soll gleich sein

nn sind sehr complex und passen sich gut an die daten an, overfitting issues
nicht zu viel freiheit geben

regularization
man kann koeffizienten eine grenze geben, damit nn sich nicht perfekt anpasst
lasso und ridge
lasso erzeugt sparse model, viele parameter auf 0 gesetzt




dimensionsreduktion

tSNE
PCA

PCA möcht Varianz erhalten, struktur kann sich ändern (wegen average werte)
tSNE lokale und globale struktur der daten erhalten
gleiche verteilung erzielen in einer niedrigen dimension

tSNE auch visuell
nachbarschaft pro punkt perplexity gute werte sind 20 bis 100
nicht deterministisch


VO
generative ai
es muss kein model sein das man trainiert
es generiert, keine prediction, regression, classification

generieren jegliche art von data

discriminative ai and generative ai
wir haben bisher discriminative ai verwendet

bsp genAI
- autoencoder (compression bsp) input = output, mit normalverteilung auf latenten space, neuer output, bisschen zufall reinstreuen
- generative adversarial network (GAN), 2 nns, 1nn versucht fälschungen zu erstellen, 1nn versucht fälschungen zu erkennen, generator and discriminator
    GAN bad for large images
- diffusion models

